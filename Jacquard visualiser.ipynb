{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy.ndimage.filters import gaussian_filter1d\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from utils.dataset_processing import evaluation, grasp\n",
    "from utils.dataset_processing.grasp import GraspRectangles, detect_grasps\n",
    "from utils.data.jacquard_sal import JacquardSalDataset\n",
    "from skimage.filters import gaussian\n",
    "from scipy.ndimage.filters import gaussian_filter1d\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img, size=224):\n",
    "    transform = T.Compose([\n",
    "        T.Resize(size),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=SQUEEZENET_MEAN.tolist(),\n",
    "                    std=SQUEEZENET_STD.tolist()),\n",
    "        T.Lambda(lambda x: x[None]),\n",
    "    ])\n",
    "    return transform(img)\n",
    "\n",
    "def deprocess(img, should_rescale=True):\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda x: x[0]),\n",
    "        T.Normalize(mean=[0, 0, 0], std=(1.0 / SQUEEZENET_STD).tolist()),\n",
    "        T.Normalize(mean=(-SQUEEZENET_MEAN).tolist(), std=[1, 1, 1]),\n",
    "        T.Lambda(rescale) if should_rescale else T.Lambda(lambda x: x),\n",
    "        T.ToPILImage(),\n",
    "    ])\n",
    "    return transform(img)\n",
    "\n",
    "def rescale(x):\n",
    "    low, high = x.min(), x.max()\n",
    "    x_rescaled = (x - low) / (high - low)\n",
    "    return x_rescaled\n",
    "    \n",
    "def blur_image(X, sigma=1):\n",
    "    X_np = X.cpu().clone().numpy()\n",
    "    X_np = gaussian_filter1d(X_np, sigma, axis=2)\n",
    "    X_np = gaussian_filter1d(X_np, sigma, axis=3)\n",
    "    X.copy_(torch.Tensor(X_np).type_as(X))\n",
    "    return X\n",
    "\n",
    "def post_process_output(q_img, cos_img, sin_img, width_img):\n",
    "    \"\"\"\n",
    "    Post-process the raw output of the GG-CNN, convert to numpy arrays, apply filtering.\n",
    "    :param q_img: Q output of GG-CNN (as torch Tensors)\n",
    "    :param cos_img: cos output of GG-CNN\n",
    "    :param sin_img: sin output of GG-CNN\n",
    "    :param width_img: Width output of GG-CNN\n",
    "    :return: Filtered Q output, Filtered Angle output, Filtered Width output\n",
    "    \"\"\"\n",
    "    q_img = q_img.detach().cpu().numpy().squeeze()\n",
    "    ang_img = (torch.atan2(sin_img, cos_img) / 2.0).detach().cpu().numpy().squeeze()\n",
    "    width_img = width_img.detach().cpu().numpy().squeeze() * 150.0\n",
    "\n",
    "    q_img = gaussian(q_img, 2.0, preserve_range=True)\n",
    "    ang_img = gaussian(ang_img, 2.0, preserve_range=True)\n",
    "    width_img = gaussian(width_img, 1.0, preserve_range=True)\n",
    "\n",
    "    return q_img, ang_img, width_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_map(X, target, y, model, loss='grasp'):\n",
    "    model.eval()\n",
    "    X.requires_grad_()\n",
    "    \n",
    "    lossd = model.compute_loss(X, target, y)\n",
    "    \n",
    "    grasploss = lossd['loss']['grasp']\n",
    "    classloss = lossd['loss']['class']\n",
    "    \n",
    "    if loss == 'grasp':\n",
    "        grasploss.backward()\n",
    "    elif loss == 'class':\n",
    "        classloss.backward()\n",
    "    elif loss == 'combined':\n",
    "        grasploss.backward(retain_graph=True)\n",
    "        classloss.backward()\n",
    "    else:\n",
    "        print('No Loss implemented')\n",
    "    \n",
    "    activation = torch.abs(X.grad)\n",
    "    activation, _ = torch.max(activation, 1)\n",
    "    activation = activation.data\n",
    "    \n",
    "    return activation\n",
    "    \n",
    "def show_activation_maps(X, target, y, model, activation='grasp'):\n",
    "    pos, cos, sin, width, pred = model(X)\n",
    "    \n",
    "    q_img, ang_img, width_img = post_process_output(pos, cos, sin, width)\n",
    "    \n",
    "    saliency = activation_map(X, target, y, model, loss=activation)\n",
    "    activation = saliency.cpu().squeeze().numpy()\n",
    "    \n",
    "    N = X.shape[0]\n",
    "    \n",
    "    for i in range(N):\n",
    "        plt.subplot(2, N, i + 1)\n",
    "        plt.imshow(X[i].detach().cpu().numpy().transpose(1,2,0))\n",
    "        plt.axis('off')\n",
    "        plt.subplot(2, N, N + i + 1)\n",
    "        plt.imshow(activation, cmap=plt.cm.hot)\n",
    "        plt.axis('off')\n",
    "        plt.gcf().set_size_inches(12, 5)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_output(img, grasp_q_img, grasp_angle_img, no_grasps=1, grasp_width_img=None):\n",
    "    \"\"\"\n",
    "    Plot the output of a GG-CNN\n",
    "    :param rgb_img: RGB Image\n",
    "    :param grasp_q_img: Q output of GG-CNN\n",
    "    :param grasp_angle_img: Angle output of GG-CNN\n",
    "    :param no_grasps: Maximum number of grasps to plot\n",
    "    :param grasp_width_img: (optional) Width output of GG-CNN\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    gs = detect_grasps(grasp_q_img, grasp_angle_img, width_img=grasp_width_img, no_grasps=no_grasps)\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(2, 2, 1)\n",
    "    ax.imshow(img)\n",
    "    for g in gs:\n",
    "        g.plot(ax)\n",
    "    ax.axis('off')\n",
    "\n",
    "    ax = fig.add_subplot(2, 2, 3)\n",
    "    plot = ax.imshow(grasp_q_img, cmap='jet', vmin=0, vmax=1)\n",
    "    ax.set_title('Q')\n",
    "    ax.axis('off')\n",
    "    plt.colorbar(plot)\n",
    "\n",
    "    ax = fig.add_subplot(2, 2, 4)\n",
    "    plot = ax.imshow(grasp_angle_img, cmap='hsv', vmin=-np.pi / 2, vmax=np.pi / 2)\n",
    "    ax.set_title('Angle')\n",
    "    ax.axis('off')\n",
    "    plt.colorbar(plot)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "topil = transforms.ToPILImage()\n",
    "\n",
    "data = JacquardSalDataset('/media/will/research/Jacquard/data', include_depth=True, include_rgb=True, train=False, random_rotate=True, random_zoom=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0603, -0.0600, -0.0596,  ...,  0.0639,  0.0644,  0.0648],\n",
       "          [-0.0603, -0.0600, -0.0596,  ...,  0.0639,  0.0644,  0.0648],\n",
       "          [-0.0603, -0.0600, -0.0596,  ...,  0.0639,  0.0644,  0.0648],\n",
       "          ...,\n",
       "          [-0.0603, -0.0600, -0.0596,  ...,  0.0639,  0.0644,  0.0648],\n",
       "          [-0.0603, -0.0600, -0.0596,  ...,  0.0639,  0.0644,  0.0648],\n",
       "          [-0.0603, -0.0600, -0.0596,  ...,  0.0639,  0.0644,  0.0648]],\n",
       " \n",
       "         [[ 0.6902,  0.6863,  0.6863,  ...,  0.6745,  0.6706,  0.6706],\n",
       "          [ 0.6902,  0.6902,  0.6902,  ...,  0.6706,  0.6706,  0.6667],\n",
       "          [ 0.6902,  0.6902,  0.6863,  ...,  0.6706,  0.6627,  0.6549],\n",
       "          ...,\n",
       "          [ 0.7176,  0.7137,  0.7137,  ...,  0.7529,  0.7529,  0.7529],\n",
       "          [ 0.7216,  0.7137,  0.7137,  ...,  0.7569,  0.7569,  0.7569],\n",
       "          [ 0.7255,  0.7216,  0.7216,  ...,  0.7569,  0.7569,  0.7569]],\n",
       " \n",
       "         [[ 0.6902,  0.6863,  0.6863,  ...,  0.6745,  0.6706,  0.6706],\n",
       "          [ 0.6902,  0.6902,  0.6902,  ...,  0.6706,  0.6706,  0.6667],\n",
       "          [ 0.6902,  0.6902,  0.6863,  ...,  0.6706,  0.6627,  0.6588],\n",
       "          ...,\n",
       "          [ 0.7176,  0.7137,  0.7137,  ...,  0.7529,  0.7529,  0.7529],\n",
       "          [ 0.7216,  0.7137,  0.7137,  ...,  0.7569,  0.7569,  0.7569],\n",
       "          [ 0.7255,  0.7216,  0.7216,  ...,  0.7569,  0.7569,  0.7569]],\n",
       " \n",
       "         [[ 0.6902,  0.6863,  0.6863,  ...,  0.6706,  0.6667,  0.6667],\n",
       "          [ 0.6902,  0.6902,  0.6863,  ...,  0.6667,  0.6627,  0.6627],\n",
       "          [ 0.6902,  0.6902,  0.6863,  ...,  0.6667,  0.6549,  0.6510],\n",
       "          ...,\n",
       "          [ 0.7176,  0.7137,  0.7137,  ...,  0.7490,  0.7490,  0.7529],\n",
       "          [ 0.7216,  0.7137,  0.7137,  ...,  0.7569,  0.7569,  0.7569],\n",
       "          [ 0.7216,  0.7216,  0.7176,  ...,  0.7569,  0.7569,  0.7569]]]),\n",
       " tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]),\n",
       " (tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]]),\n",
       "  tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           ...,\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.]]]),\n",
       "  tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]]),\n",
       "  tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]])),\n",
       " 0,\n",
       " 4.71238898038469,\n",
       " 0.8839369146426914)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
